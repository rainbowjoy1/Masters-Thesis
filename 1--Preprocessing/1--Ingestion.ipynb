{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielleDuncan\\AppData\\Local\\Temp\\ipykernel_18448\\1079661689.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  article_df_4[\"news_post_date\"] = article_df_4.news_post_date.str.replace(\"(T).*\",\"\")\n"
     ]
    }
   ],
   "source": [
    "#Ingestion json file\n",
    "\n",
    "filejson = r\"C:\\Users\\DanielleDuncan\\Desktop\\THESIS\\thesis dataset\\bbc_latest_news_dataset_2021.json\"\n",
    "article_df_4 = pd.read_json(filejson)\n",
    "article_df_4 = article_df_4.drop([\"tags\", \"raw_content\", \"language\", \"_id\", \"category\", \"crawled_at\", \"short_description\"], axis=1)\n",
    "article_df_4 = article_df_4[['title', 'url', 'news_post_date', 'author', 'region', 'content']]\n",
    "article_df_4[\"news_post_date\"] = article_df_4.news_post_date.str.replace(\"(T).*\",\"\")\n",
    "article_df_4.rename(columns = {'news_post_date':'published_at', 'region':'category','content':'description'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion\n",
    "\n",
    "#Actual Dataset--File 1\n",
    "\n",
    "filecsv1 = r\"C:\\Users\\DanielleDuncan\\Desktop\\THESIS\\thesis dataset\\bbc_news_01.csv\"\n",
    "article_df_1 = pd.read_csv(filecsv1)\n",
    "\n",
    "#Actual Dataset--File 2\n",
    "\n",
    "filecsv2 = r\"C:\\Users\\DanielleDuncan\\Desktop\\THESIS\\thesis dataset\\bbc_news_02.csv\"\n",
    "article_df_2 = pd.read_csv(filecsv2)\n",
    "\n",
    "#Actual Dataset--File 3\n",
    "filecsv3 = r\"C:\\Users\\DanielleDuncan\\Desktop\\THESIS\\thesis dataset\\bbc_news_03.csv\"\n",
    "article_df_3 = pd.read_csv(filecsv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.concat([article_df_1, article_df_2, article_df_3, article_df_4])\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df = article_df.drop([\"publisher\", \"header_image\", \"index\", \"raw_description\", \"short_description\", \"uniq_id\", \"scraped_at\"], axis=1)\n",
    "year = article_df['published_at'].str[:4]\n",
    "article_df['year']=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "\n",
    "def split_sentences(article, article_id, year):\n",
    "    pattern = r'(?<=[a-z0-9\"]) *[.?!] *(?=[A-Z])'\n",
    "    article = re.sub(pattern, r'\\g<0> ', article)\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id, year) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id, year in article_df[['description','Article_Number', 'year']].values:\n",
    "    sentences = split_sentences(str(article), article_id, year)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/DanielleDuncan/Desktop/THESIS/raw data/\"\n",
    "#2009\n",
    "df_09 = sentences_df[sentences_df['year'] == '2009']\n",
    "with open(path + '2009_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_09, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2010\n",
    "df_10 = sentences_df[sentences_df['year'] == '2010']\n",
    "with open(path + '2010_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_10, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2011\n",
    "df_11 = sentences_df[sentences_df['year'] == '2011']\n",
    "with open(path + '2011_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_11, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2012\n",
    "df_12 = sentences_df[sentences_df['year'] == '2012']\n",
    "with open(path + '2012_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_12, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2013\n",
    "df_13 = sentences_df[sentences_df['year'] == '2013']\n",
    "with open(path + '2013_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_13, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2014\n",
    "df_14 = sentences_df[sentences_df['year'] == '2014']\n",
    "with open(path + '2014_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_14, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2015\n",
    "df_15 = sentences_df[sentences_df['year'] == '2015']\n",
    "with open(path + '2015_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_15, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2016\n",
    "df_16 = sentences_df[sentences_df['year'] == '2016']\n",
    "with open(path + '2016_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_16, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2017\n",
    "df_17 = sentences_df[sentences_df['year'] == '2017']\n",
    "with open(path + '2017_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_17, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2018\n",
    "df_18 = sentences_df[sentences_df['year'] == '2018']\n",
    "with open(path + '2018_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_18, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2019\n",
    "df_19 = sentences_df[sentences_df['year'] == '2019']\n",
    "with open(path + '2019_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_19, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2020\n",
    "df_20 = sentences_df[sentences_df['year'] == '2020']\n",
    "with open(path + '2020_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_20, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#20121\n",
    "df_21 = sentences_df[sentences_df['year'] == '2021']\n",
    "with open(path + '2021_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_21, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#2022\n",
    "df_22 = sentences_df[sentences_df['year'] == '2022']\n",
    "with open(path + '2022_raw_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_22, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
