{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from genderize import Genderize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#Variables(for now):\n",
    "name_probability_list = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion json file\n",
    "\n",
    "#filejson = r\"C:\\Users\\danie\\Desktop\\bbc_latest_news_dataset_2021.json\"\n",
    "\n",
    "#article_df_4 = pd.read_json(filejson)\n",
    "#article_df_4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion\n",
    "\n",
    "#Actual Dataset--File 1\n",
    "\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_01.csv\"\n",
    "article_df_1 = pd.read_csv(filecsv)\n",
    "\n",
    "#Actual Dataset--File 2\n",
    "\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_02.csv\"\n",
    "article_df_2 = pd.read_csv(filecsv)\n",
    "\n",
    "#Actual Dataset--File 3\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_03.csv\"\n",
    "article_df_3 = pd.read_csv(filecsv)\n",
    "\n",
    "#Merge Datasets\n",
    "\n",
    "article_df = pd.concat([article_df_1, article_df_2, article_df_3])\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df.drop([\"publisher\", \"header_image\", \"index\", \"raw_description\", \"short_description\", \"uniq_id\", \"scraped_at\"], axis=1)\n",
    "year = article_df['published_at'].str[:4]\n",
    "article_df['year']=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "\n",
    "def split_sentences(article, article_id, year):\n",
    "    pattern = r'(?<=[a-z0-9\"]) *[.?!] *(?=[A-Z])'\n",
    "    article = re.sub(pattern, r'\\g<0> ', article)\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id, year) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id, year in article_df[['description','Article_Number', 'year']].values:\n",
    "    sentences = split_sentences(str(article), article_id, year)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df\n",
    "#add method to divied DF into sperate ones by year\n",
    "grouped = sentences_df.groupby(['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_09 = sentences_df[sentences_df['year'] == '2009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing (sentence):\n",
    "    Male_count = 0\n",
    "    Female_count = 0\n",
    "    APIcallfail= 0\n",
    "\n",
    "#regex_cleanup\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    #remove 's\n",
    "\n",
    "#tokenize\n",
    "    sentence =  nltk.TweetTokenizer().tokenize(sentence)\n",
    "\n",
    "#tag_and_stem\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence)\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    pn_tags = {'NNP', 'NNPS'}\n",
    "\n",
    "    new_words = []\n",
    "    proper_nouns = []\n",
    "\n",
    "    for word, tag in tagged_sentence: \n",
    "        if tag not in pn_tags: \n",
    "            if tag.startswith(\"V\"):\n",
    "                lemmas = lemma.lemmatize(word, \"v\")\n",
    "            else: \n",
    "                lemmas = lemma.lemmatize(word)\n",
    "            new_words.append((lemmas))\n",
    "        else:\n",
    "            proper_nouns.append([word, tag])\n",
    "            #i think the namesshould be dropped.\n",
    "    sentence = new_words\n",
    "\n",
    "#name_gender\n",
    "    #nltk_results = ne_chunk(tagged_sentence)\n",
    "    nltk_results = ne_chunk(proper_nouns)\n",
    "\n",
    "    for nltk_result in nltk_results:\n",
    "        if type(nltk_result) == Tree:\n",
    "            name = ''\n",
    "            for nltk_result_leaf in nltk_result.leaves():\n",
    "                name += nltk_result_leaf[0] + ' '\n",
    "            if nltk_result.label() == \"PERSON\":\n",
    "                name = name.split(' ')[0]\n",
    "                try: \n",
    "                    word_gender = name_probability_list.get(name)\n",
    "                    if word_gender is None:\n",
    "                        word_gender = Genderize().get1(name).get('gender')\n",
    "                        name_probability_list[name] = word_gender\n",
    "                    if word_gender == 'male':\n",
    "                        Male_count += 1\n",
    "                    if word_gender== 'female':\n",
    "                        Female_count += 1\n",
    "                except Exception as exception:\n",
    "                    APIcallfail +=1\n",
    "            else: \n",
    "                sentence.append(name) #test this out!\n",
    "    \n",
    "#Lower\n",
    "    sentence = [x.lower() for x in sentence]\n",
    "\n",
    "\n",
    "#contractions\n",
    "    new_text = []\n",
    "    for word in new_words:\n",
    "        contraction = contractions.get(word)\n",
    "        if contraction is None:\n",
    "            new_text.append(word)\n",
    "        else:\n",
    "            for word in contraction.split():\n",
    "                new_text.append(word)\n",
    "    sentence = new_text\n",
    "\n",
    "#gendered_count\n",
    "    for w in sentence:\n",
    "        if w in male_list:\n",
    "            Male_count += 1\n",
    "        if w in female_list:\n",
    "            Female_count += 1\n",
    "\n",
    "#remove_stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    sentence = [w for w in sentence if not w in stops]\n",
    "\n",
    "#remove_leakage\n",
    "    new_sent = [x for x in sentence if x not in male_list]\n",
    "    new_sent = [x for x in new_sent if x not in female_list]\n",
    "    sentence = new_sent\n",
    "\n",
    "    #remove words smaller than 2\n",
    "\n",
    "    print(sentence)\n",
    "    return sentence, Male_count, Female_count, APIcallfail\n",
    "\n",
    "#if still having memory issues we needd to chunk the dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_09['encoded_sentences'] = df_09['sentences'].apply(PreProcessing)\n",
    "\n",
    "#sentences_df['encoded_sentences'] = sentences_df['sentences'].apply(PreProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_09_split = pd.DataFrame(df_09[\"encoded_sentences\"].to_list(), columns=['pre_processed_sent','male_count','female_count','apicall_fail'])\n",
    "result = pd.concat([df_09_split, (df_09.reset_index(drop=True))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(\"encoded_sentences\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[9546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = result.pre_processed_sent.apply(lambda x: 'he' in x)\n",
    "df1 = result[mask]\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the api calls!!!\n",
    "with open('2009_preprocessed_date.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_09, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_split = pd.DataFrame(sample_df[\"encoded_sentences\"].to_list(), columns=['pre_processed_sent','male_count','female_count','apicall_fail'])\n",
    "#result = pd.concat([sample_df, sample_df_split], ignore_index=True, sort=False)\n",
    "#result = pd.concat([sample_df, sample_df_split.reindex(sample_df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the api calls!!!\n",
    "with open('name_probability_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(name_probability_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCE LISTS--TO BE CHANGED\n",
    "male_list= {\"man\", \"men\", \"mister\", \"he\", \"him\", \"Mr.\", \"he\", \"his\",\"he's\", \"hes\", \"father\", \"dad\", \"daddy\", \"grandpa\", \"grandfather\", \"husband\"}\n",
    "female_list ={\"woman\", \"women\", \"missus\", \"misses\", \"Ms.\", \"Mrs.\", \"her\", \"she\", \"hers\", \"mother\", \"mom\", \"mommy\", \"aunt\", \"grandmother\", \"grandma\", \"wife\", \"wive\"}\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(male_col, female_col): \n",
    "    \"\"\"This function compares the count of female to male pronouns. It will output \"1\" if male count bigger\n",
    "    than female count, \"neutral\" if the count is equal, and \"female\" if there is a higher female count. \n",
    "    The function returns strings because we need categorical variables for log reg to run\"\"\"\n",
    "    if female_col > male_col:\n",
    "        return \"0\"\n",
    "    elif male_col > female_col:\n",
    "        return \"1\"\n",
    "    else: \n",
    "        return None\n",
    "sentences_df['col_type'] = sentences_df.apply(lambda row: compare_count(row['male_count'], row['female_count']),axis=1)\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with \"None\" in the col_type \n",
    "sentences_df = sentences_df[sentences_df[\"col_type\"].notnull()]\n",
    "#sentences_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
