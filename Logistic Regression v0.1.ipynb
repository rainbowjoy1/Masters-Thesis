{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -n base ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'articles.pkl'\n",
    "\n",
    "article_df = pd.read_pickle(filename)\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "def split_sentences(article, article_id):\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id in article_df[['content','Article_Number']].values:\n",
    "    sentences = split_sentences(str(article), article_id)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id'])\n",
    "\n",
    "# TO-DO: add the data & find the pronouns to determine gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_df.drop(['female_count', 'male_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_w = [' his ', ' he ']\n",
    "his_w_expand = [\"his\", \"he\", \"man\", \"uncle\", \"dad\", \"daddy\", \"father\", \"boy\", \"husband\"]\n",
    "her_w = [\" her \", \" she \"]\n",
    "her_w_expand = [\"her\", \"she\", \"woman\", \"aunt\", \"mother\",\"mom\", \"mommy\", \"girl\", \"wife\"]\n",
    "\n",
    "\n",
    "def pronoun_occurance(text, female_list, male_list):\n",
    "    female_count = sum(text.count(word) for word in female_list)\n",
    "    male_count = sum(text.count(word) for word in male_list)\n",
    "    return female_count, male_count\n",
    "\n",
    "#sentences_df['female_count'], sentences_df['male_count'] = zip(sentences_df['sentences'].apply(pronoun_occurance, female_list=her_w, male_list=his_w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this solution fixes the bug of double counting \n",
    "def pronoun_occurances(text):\n",
    "    \"\"\" This function will count the number of female and male pronoun occurences in a given sentence. \n",
    "    We will need to update the REGEX in order to incorporate more pronouns if we need to\"\"\"\n",
    "    pattern_m = r'(\\s|^)(he|his)\\b' #this regex will capture he/his as standalone words within a string but also at beginning of sentence\n",
    "    matches_m = re.findall(pattern_m, text, re.IGNORECASE) #IGNORECASE is necessary to make sure that it picks up the pronouns at the beginning of a sentence\n",
    "    pattern_f = r'(\\s|^)(she|her)\\b'\n",
    "    matches_f = re.findall(pattern_f, text, re.IGNORECASE)\n",
    "    count_m = len(matches_m)\n",
    "    count_f = len(matches_f)\n",
    "    return count_f, count_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable applying the function of pronoun occurence\n",
    "sent = sentences_df['sentences'].apply(pronoun_occurances)\n",
    "# Create two new columns in sentences DF from the tuple output in \"sent\"\n",
    "sentences_df['female_count'] = [x[0] for x in sent]\n",
    "sentences_df['male_count']= [x[1] for x in sent]\n",
    "\n",
    "#Bug is fixed and now it counts properly\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.loc[[7]] #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(male_col, female_col): \n",
    "    \"\"\"This function compares the count of female to male pronouns. It will output \"1\" if male count bigger\n",
    "    than female count, \"neutral\" if the count is equal, and \"female\" if there is a higher female count. \n",
    "    The function returns strings because we need categorical variables for log reg to run\"\"\"\n",
    "    if male_col > female_col: \n",
    "        return \"1\"\n",
    "    elif male_col == female_col: \n",
    "        return \"0.5\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "sentences_df['col_type'] = sentences_df.apply(lambda row: compare_count(row['male_count'], row['female_count']), axis=1)\n",
    "sentences_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics Regression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Encoding\n",
    "\n",
    "sentences_df['encoded_sentences'] = sentences_df.loc[:,'sentences']\n",
    "\n",
    "def tidy_text(sentence):\n",
    "   \n",
    "    # Convert words to lower case\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        sentence = sentence.split()\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        sentence = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r'\\'', ' ', sentence)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        sentence = sentence.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        sentence = [w for w in sentence if not w in stops]\n",
    "        sentence = \" \".join(sentence)\n",
    "\n",
    "    # Tokenize each word\n",
    "    sentence =  nltk.WordPunctTokenizer().tokenize(sentence)\n",
    "    \n",
    "    # Lemmatize each token\n",
    "    lemm = nltk.stem.WordNetLemmatizer()\n",
    "    sentence = list(map(lambda word:list(map(lemm.lemmatize, word)), sentence))\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "sentence_df['encoded_sentences'] = sentence_df['encoded_sentences'].apply(tidy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc)\n",
    "\n",
    "#this should be changed to only the male and female sents. \n",
    "x = bow_converter.fit_transform(sentence_df['encoded_sentences'])\n",
    "words = bow_converter.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training and test set \n",
    "X = sentences_df[[\"female_count\", \"male_count\"]] #input \n",
    "y = sentences_df[\"col_type\"] #label \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "#Note: using sentences_df[\"sentences\"] creates an error \n",
    "\n",
    "# I think we are getting an error here because we haven't encoded the sentences. I'm not sure if I should work on this or the data set. I'm thinking BOW \n",
    "#and TFIDF for what we should use to encode. Also I do think we need to clean up the sentences. ie punctuation, lowercase, contractions, tokenazion, \n",
    "# and maybe stemming? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial logistic regression \n",
    "logreg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\") #classifier \n",
    "logreg.fit(X_train, y_train) #fit the model \n",
    "logreg.score(X_train, y_train) #get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(X_train.columns, np.transpose(abs(logreg.coef_))), columns=['features', 'coef']) #use absolute values to identify biggest coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, male coefficient for neutral is higher than the female. Otherwise, as expected, the coefficients for female and male each are correspondingly high for each gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logreg.predict(X_test) #predict test \n",
    "print(metrics.accuracy_score(y_test, prediction)) #accuracy \n",
    "print(metrics.confusion_matrix(y_test, prediction)) #confusion matrix\n",
    "print(metrics.classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think with the above solution we wouldn't need this function \n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "sentences_df['male_count2'] = sentences_df['sentences'].apply(count_words, word_list=his_w)\n",
    "sentences_df['female_count2'] = sentences_df['sentences'].apply(count_words, word_list=her_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "sentences_df.loc[[7]]\n",
    "\n",
    "#there is a bug here. It weirdly seems to be double counting? The zip function is new to me though so maybe thats the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': ['This is a sample text', 'Another text example', 'One more example']})\n",
    "\n",
    "# define two lists of specific words to count\n",
    "word_list1 = ['text', 'example']\n",
    "word_list2 = ['is', 'more']\n",
    "\n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "# use apply() to add two new columns with the counts of the specific words in each list\n",
    "df['word_count1'] = df['text'].apply(count_words, word_list=word_list1)\n",
    "df['word_count2'] = df['text'].apply(count_words, word_list=word_list2)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
