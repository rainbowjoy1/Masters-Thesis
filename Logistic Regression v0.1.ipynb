{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Json file option\n",
    "filejson = \"C:/Users/danie/Desktop/bbc_news_list_uk.json\"\n",
    "filecsv = \"C:/Users/danie/Desktop/bbc_news_list_uk.csv\"\n",
    "article_df = pd.read_csv(filecsv)\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code assumes the first four digits are the year. can be changed for last of middle\n",
    "year = article_df['news_post_date'].str[:4]\n",
    "article_df['year']=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'articles.pkl'\n",
    "\n",
    "#article_df = pd.read_pickle(filename)\n",
    "#article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "#article_df = article_df.reset_index()\n",
    "#article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "import re\n",
    "\n",
    "def split_sentences(article, article_id, year):\n",
    "    pattern = r'(?<=[a-z0-9\"]) *[.?!] *(?=[A-Z])'\n",
    "    article = re.sub(pattern, r'\\g<0> ', article)\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id, year) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id, year in article_df[['content','Article_Number', 'year']].values:\n",
    "    sentences = split_sentences(str(article), article_id, year)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun_occurances(text):\n",
    "    \"\"\" This function will count the number of female and male pronoun occurences in a given sentence. \n",
    "    We will need to update the REGEX in order to incorporate more pronouns if we need to\"\"\"\n",
    "    pattern_m = r'(\\s|^)(he|his|him|he\\'s|hes)\\b' #this regex will capture he/his as standalone words within a string but also at beginning of sentence\n",
    "    matches_m = re.findall(pattern_m, text, re.IGNORECASE) #IGNORECASE is necessary to make sure that it picks up the pronouns at the beginning of a sentence\n",
    "    pattern_f = r'(\\s|^)(she|her|hers|shes|she\\'s)\\b'\n",
    "    matches_f = re.findall(pattern_f, text, re.IGNORECASE)\n",
    "    count_m = len(matches_m)\n",
    "    count_f = len(matches_f)\n",
    "    return count_f, count_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable applying the function of pronoun occurence\n",
    "sent = sentences_df['sentences'].apply(pronoun_occurances)\n",
    "# Create two new columns in sentences DF from the tuple output in \"sent\"\n",
    "sentences_df['female_count'] = [x[0] for x in sent]\n",
    "sentences_df['male_count']= [x[1] for x in sent]\n",
    "\n",
    "#Bug is fixed and now it counts properly\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(male_col, female_col): \n",
    "    \"\"\"This function compares the count of female to male pronouns. It will output \"1\" if male count bigger\n",
    "    than female count, \"neutral\" if the count is equal, and \"female\" if there is a higher female count. \n",
    "    The function returns strings because we need categorical variables for log reg to run\"\"\"\n",
    "    if male_col > female_col: \n",
    "        return \"2\"\n",
    "    elif male_col == female_col: \n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "sentences_df['col_type'] = sentences_df.apply(lambda row: compare_count(row['male_count'], row['female_count']), axis=1)\n",
    "sentences_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics Regression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Encoding\n",
    "\n",
    "sentences_df['encoded_sentences'] = sentences_df.loc[:,'sentences']\n",
    "\n",
    "def tidy_text(sentence, remove_stopwords = True):\n",
    "\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence)\n",
    "        #changed the number detection code\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r'\\'', ' ', sentence)\n",
    "\n",
    "        # Tokenize each word\n",
    "    sentence =  nltk.WordPunctTokenizer().tokenize(sentence)\n",
    "\n",
    "    nltk.tag.pos_tag(sentence.split())\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    sentence = (' '.join(edited_sentence))\n",
    "\n",
    "    # Convert words to lower case\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        sentence = sentence.split()\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        sentence = \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        sentence = sentence.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        sentence = [w for w in sentence if not w in stops]\n",
    "        sentence = \" \".join(sentence)\n",
    "\n",
    "\n",
    "    \n",
    "    # Lemmatize each token\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "    #Maybe we should remove names? At least (could just be proper nouns)\n",
    "\n",
    "sentences_df['encoded_sentences'] = sentences_df['encoded_sentences'].apply(tidy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter loop the DT\n",
      "enter loop bloodied JJ\n",
      "new tag a\n",
      "lematized word with POS bloodied\n",
      "lemma ['the', 'bloodied']\n",
      "enter loop person NN\n",
      "new tag n\n",
      "lematized word with POS person\n",
      "lemma ['the', 'bloodied', 'person']\n",
      "enter loop saw VBD\n",
      "enter loop some DT\n",
      "enter loop blood NN\n",
      "new tag n\n",
      "lematized word with POS blood\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood']\n",
      "enter loop drawing VBG\n",
      "enter loop draws JJ\n",
      "new tag a\n",
      "lematized word with POS draws\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood', 'drawing', 'draws']\n",
      "enter loop drew JJ\n",
      "new tag a\n",
      "lematized word with POS drew\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood', 'drawing', 'draws', 'drew']\n",
      "enter loop draw NN\n",
      "new tag n\n",
      "lematized word with POS draw\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood', 'drawing', 'draws', 'drew', 'draw']\n",
      "enter loop running VBG\n",
      "enter loop runs NNS\n",
      "enter loop run VBP\n",
      "enter loop change NN\n",
      "new tag n\n",
      "lematized word with POS change\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood', 'drawing', 'draws', 'drew', 'draw', 'running', 'runs', 'run', 'change']\n",
      "enter loop changing NN\n",
      "new tag n\n",
      "lematized word with POS changing\n",
      "lemma ['the', 'bloodied', 'person', 'saw', 'some', 'blood', 'drawing', 'draws', 'drew', 'draw', 'running', 'runs', 'run', 'change', 'changing']\n",
      "enter loop changes NNS\n",
      "enter loop change VBP\n",
      "enter loop Professor NNP\n",
      "enter loop John NNP\n",
      "enter loop in IN\n",
      "enter loop Denmark NNP\n",
      "enter loop with IN\n",
      "enter loop Leo NNP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bloodied',\n",
       " 'person',\n",
       " 'saw',\n",
       " 'blood',\n",
       " 'drawing',\n",
       " 'draws',\n",
       " 'drew',\n",
       " 'draw',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'run',\n",
       " 'change',\n",
       " 'changing',\n",
       " 'changes',\n",
       " 'change']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "#Sentence Encoding\n",
    "\n",
    "def tidy_text(sentence, remove_stopwords = True):\n",
    "\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence)\n",
    "        #changed the number detection code\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r'\\'', ' ', sentence)\n",
    "\n",
    "        # Tokenize each word\n",
    "    sentence =  nltk.WordPunctTokenizer().tokenize(sentence)\n",
    "\n",
    "    nltk.tag.pos_tag(sentence)\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence)\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    lemma_tags = ('NN', 'VB', 'JJ', 'RB')\n",
    "    pn_tags = ('NNP', 'NNPS')\n",
    "\n",
    "    def POS_tag_lookup(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "\n",
    "    new_words = [] \n",
    "    for word, tag in tagged_sentence:\n",
    "        print(\"enter loop\",word, tag)\n",
    "        if tag not in pn_tags:\n",
    "            if tag in lemma_tags:\n",
    "                new_tag = POS_tag_lookup(tag)\n",
    "                print(\"new tag\", new_tag)\n",
    "                new_word = (lemma.lemmatize(word, new_tag))\n",
    "                print(\"lematized word with POS\", new_word)\n",
    "                new_words.append(new_word)\n",
    "                print(\"lemma\", new_words)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "\n",
    "    sentence = new_words\n",
    "\n",
    "\n",
    "    #sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    #print(sentence)\n",
    "\n",
    "\n",
    "    #sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    #return sentence\n",
    "\n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "    \n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        sentence = [w for w in sentence if not w in stops]\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "text = \"the bloodied person saw some blood drawing draws drew draw running runs run change changing changes change Professor John in Denmark with Leo\"\n",
    "tidy_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemma test\n",
    "\n",
    "text = \"the bloodied person saw some blood drawing draws drew draw running runs run change changing changes change\"\n",
    "nltk.tag.pos_tag(text.split())\n",
    "tagged_sentence = nltk.tag.pos_tag(text.split())\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ, #adjective\n",
    "    \"N\": wordnet.NOUN,#noun\n",
    "    \"V\": wordnet.VERB,#verb\n",
    "    \"R\": wordnet.ADV} #adverb\n",
    "\n",
    "return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  text=[WordNetLemmatizer().lemmatize(w, get_pos_tags(w)) for w in text]   \n",
    "  return text\n",
    "\n",
    "final_output=lemmatize_text(example)\n",
    "print (final_output)\n",
    "    \n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    sentence = (' '.join(edited_sentence))\n",
    "\n",
    "text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "sentence = [lemma.lemmatize(word) for word in text]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(sentences_df.year)\n",
    "rated_dummies = pd.get_dummies((sentences_df).year)\n",
    "sentences_df = pd.concat([sentences_df, rated_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = sentences_df[['encoded_sentences', '2010', '2012']]\n",
    "#y = sentences_df[\"col_type\"]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences_df['encoded_sentences']\n",
    "y = sentences_df['col_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(max_features= 1000, lowercase=False, tokenizer=False)\n",
    "def fake(token):\n",
    "    return token\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fake,\n",
    "    preprocessor=fake,\n",
    "    token_pattern=None)  \n",
    "\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial logistic regression \n",
    "logreg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", max_iter= 5000) #classifier \n",
    "logreg.fit(X_train, y_train) #fit the model \n",
    "logreg.score(X_train, y_train) #get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = logreg.coef_[0]\n",
    "#male is 2 and female is 0\n",
    "\n",
    "sorted_coef = sorted((zip(tfidf.get_feature_names(), coefs)), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "high_coef = sorted_coef[:10]\n",
    "low_coef = sorted_coef[-10:]\n",
    "\n",
    "print(\"highest coefs\")\n",
    "for i in high_coef: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lowest coefs\")\n",
    "for i in low_coef: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(X_train.columns, np.transpose(abs(logreg.coef_))), columns=['features', 'coef']) #use absolute values to identify biggest coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, male coefficient for neutral is higher than the female. Otherwise, as expected, the coefficients for female and male each are correspondingly high for each gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logreg.predict(X_test) #predict test \n",
    "print(metrics.accuracy_score(y_test, prediction)) #accuracy \n",
    "print(metrics.confusion_matrix(y_test, prediction)) #confusion matrix\n",
    "print(metrics.classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think with the above solution we wouldn't need this function \n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "sentences_df['male_count2'] = sentences_df['sentences'].apply(count_words, word_list=his_w)\n",
    "sentences_df['female_count2'] = sentences_df['sentences'].apply(count_words, word_list=her_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "sentences_df.loc[[7]]\n",
    "\n",
    "#there is a bug here. It weirdly seems to be double counting? The zip function is new to me though so maybe thats the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': ['This is a sample text', 'Another text example', 'One more example']})\n",
    "\n",
    "# define two lists of specific words to count\n",
    "word_list1 = ['text', 'example']\n",
    "word_list2 = ['is', 'more']\n",
    "\n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "# use apply() to add two new columns with the counts of the specific words in each list\n",
    "df['word_count1'] = df['text'].apply(count_words, word_list=word_list1)\n",
    "df['word_count2'] = df['text'].apply(count_words, word_list=word_list2)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
