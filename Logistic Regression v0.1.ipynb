{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yolandaferreirofranchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15825 entries, 0 to 15824\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              15825 non-null  int64 \n",
      " 1   tags               1 non-null      object\n",
      " 2   title              15825 non-null  object\n",
      " 3   news_post_date     15825 non-null  object\n",
      " 4   raw_content        15468 non-null  object\n",
      " 5   content            15468 non-null  object\n",
      " 6   url                15825 non-null  object\n",
      " 7   author             1453 non-null   object\n",
      " 8   language           15825 non-null  object\n",
      " 9   id                 15825 non-null  object\n",
      " 10  region             15488 non-null  object\n",
      " 11  short_description  15825 non-null  object\n",
      " 12  category           15825 non-null  object\n",
      " 13  crawled_at         15825 non-null  object\n",
      " 14  Article_Number     15825 non-null  int64 \n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#Json file option\n",
    "#filejson = \"C:/Users/danie/Desktop/bbc_news_list_uk.json\"\n",
    "#filecsv = r\"C:\\Users\\danie\\Documents\\GitHub\\Masters-Thesis\\bbc_news_list_uk.csv\"\n",
    "filecsv = r\"/Users/yolandaferreirofranchi/Documents/GitHub/Masters-Thesis/bbc_news_list_uk.csv\"\n",
    "article_df = pd.read_csv(filecsv)\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code assumes the first four digits are the year. can be changed for last of middle\n",
    "year = article_df['news_post_date'].str[:4]\n",
    "article_df['year']=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'articles.pkl'\n",
    "\n",
    "#article_df = pd.read_pickle(filename)\n",
    "#article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "#article_df = article_df.reset_index()\n",
    "#article_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "import re\n",
    "\n",
    "def split_sentences(article, article_id, year):\n",
    "    pattern = r'(?<=[a-z0-9\"]) *[.?!] *(?=[A-Z])'\n",
    "    article = re.sub(pattern, r'\\g<0> ', article)\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id, year) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id, year in article_df[['content','Article_Number', 'year']].values:\n",
    "    sentences = split_sentences(str(article), article_id, year)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun_occurances(text):\n",
    "    \"\"\" This function will count the number of female and male pronoun occurences in a given sentence. \n",
    "    We will need to update the REGEX in order to incorporate more pronouns if we need to\"\"\"\n",
    "    pattern_m = r'(\\s|^)(he|his|him|he\\'s|hes)\\b' #this regex will capture he/his as standalone words within a string but also at beginning of sentence\n",
    "    matches_m = re.findall(pattern_m, text, re.IGNORECASE) #IGNORECASE is necessary to make sure that it picks up the pronouns at the beginning of a sentence\n",
    "    pattern_f = r'(\\s|^)(she|her|hers|shes|she\\'s)\\b'\n",
    "    matches_f = re.findall(pattern_f, text, re.IGNORECASE)\n",
    "    pattern_n = r'(\\s|^)(they|them|their|theirs|their\\'s)\\b'\n",
    "    matches_n = re.findall(pattern_f, text, re.IGNORECASE)\n",
    "    count_m = len(matches_m)\n",
    "    count_f = len(matches_f)\n",
    "    count_n = len(matches_n)\n",
    "    count_u = count_n + count_f + count_m\n",
    "    return count_f, count_m, count_n, count_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>article_id</th>\n",
       "      <th>year</th>\n",
       "      <th>female_count</th>\n",
       "      <th>male_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>u_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The heroin substitute methadone can be used as...</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earlier this year a debate broke out in Scotla...</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But a group of 40 specialists, including unive...</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So what do recovering addicts think?</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris used methadone for five years to help we...</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225660</th>\n",
       "      <td>Similar moves in Europe have sparked cries of ...</td>\n",
       "      <td>15824</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225661</th>\n",
       "      <td>The lead in the Daily Mail is a claim that cou...</td>\n",
       "      <td>15824</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225662</th>\n",
       "      <td>It says authorities are using the information ...</td>\n",
       "      <td>15824</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225663</th>\n",
       "      <td>The Daily Express leads with a warning from US...</td>\n",
       "      <td>15824</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225664</th>\n",
       "      <td>But the paper says cancer experts in the UK ha...</td>\n",
       "      <td>15824</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225665 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentences  article_id  year  \\\n",
       "0       The heroin substitute methadone can be used as...           0  2010   \n",
       "1       Earlier this year a debate broke out in Scotla...           0  2010   \n",
       "2       But a group of 40 specialists, including unive...           0  2010   \n",
       "3                    So what do recovering addicts think?           0  2010   \n",
       "4       Chris used methadone for five years to help we...           0  2010   \n",
       "...                                                   ...         ...   ...   \n",
       "225660  Similar moves in Europe have sparked cries of ...       15824  2010   \n",
       "225661  The lead in the Daily Mail is a claim that cou...       15824  2010   \n",
       "225662  It says authorities are using the information ...       15824  2010   \n",
       "225663  The Daily Express leads with a warning from US...       15824  2010   \n",
       "225664  But the paper says cancer experts in the UK ha...       15824  2010   \n",
       "\n",
       "        female_count  male_count  neutral_count  u_count  \n",
       "0                  0           0              0        0  \n",
       "1                  0           0              0        0  \n",
       "2                  0           0              0        0  \n",
       "3                  0           0              0        0  \n",
       "4                  0           3              0        3  \n",
       "...              ...         ...            ...      ...  \n",
       "225660             0           0              0        0  \n",
       "225661             0           0              0        0  \n",
       "225662             0           0              0        0  \n",
       "225663             0           0              0        0  \n",
       "225664             0           0              0        0  \n",
       "\n",
       "[225665 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a variable applying the function of pronoun occurence\n",
    "sent = sentences_df['sentences'].apply(pronoun_occurances)\n",
    "# Create two new columns in sentences DF from the tuple output in \"sent\"\n",
    "sentences_df['female_count'] = [x[0] for x in sent]\n",
    "sentences_df['male_count']= [x[1] for x in sent]\n",
    "sentences_df['neutral_count']= [x[2] for x in sent]\n",
    "sentences_df['u_count']= [x[3] for x in sent]\n",
    "\n",
    "#Bug is fixed and now it counts properly\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(male_col, female_col): \n",
    "    \"\"\"This function compares the count of female to male pronouns. It will output \"1\" if male count bigger\n",
    "    than female count, \"neutral\" if the count is equal, and \"female\" if there is a higher female count. \n",
    "    The function returns strings because we need categorical variables for log reg to run\"\"\"\n",
    "    if male_col > female_col: \n",
    "        return \"2\"\n",
    "    elif male_col == female_col: \n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "sentences_df['col_type'] = sentences_df.apply(lambda row: compare_count(row['male_count'], row['female_count']), axis=1)\n",
    "sentences_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistics Regression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence Encoding\n",
    "\n",
    "sentences_df['encoded_sentences'] = sentences_df.loc[:,'sentences']\n",
    "\n",
    "def tidy_text(sentence, remove_stopwords = True):\n",
    "\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence)\n",
    "        #changed the number detection code\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r'\\'', ' ', sentence)\n",
    "\n",
    "        # Tokenize each word\n",
    "    sentence =  nltk.WordPunctTokenizer().tokenize(sentence)\n",
    "\n",
    "    nltk.tag.pos_tag(sentence.split())\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    sentence = (' '.join(edited_sentence))\n",
    "\n",
    "    # Convert words to lower case\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        sentence = sentence.split()\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        sentence = \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        sentence = sentence.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        sentence = [w for w in sentence if not w in stops]\n",
    "        sentence = \" \".join(sentence)\n",
    "\n",
    "\n",
    "    \n",
    "    # Lemmatize each token\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "    #Maybe we should remove names? At least (could just be proper nouns)\n",
    "\n",
    "sentences_df['encoded_sentences'] = sentences_df['encoded_sentences'].apply(tidy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Sentence Encoding\n",
    "def tidy_text(sentence, remove_stopwords = True):\n",
    "\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence)\n",
    "    \n",
    "    #changed the number detection code\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r'\\'', ' ', sentence)\n",
    "\n",
    "        # Tokenize each word\n",
    "    sentence =  nltk.WordPunctTokenizer().tokenize(sentence)\n",
    "\n",
    "    nltk.tag.pos_tag(sentence)\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence)\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    pn_tags = ('NNP', 'NNPS')\n",
    "    new_words = []\n",
    "\n",
    "    for word, tag in tagged_sentence: \n",
    "        if tag not in pn_tags: \n",
    "            if tag.startswith(\"V\"):\n",
    "                lemmas = lemma.lemmatize(word, \"v\")\n",
    "            else: \n",
    "                lemmas = lemma.lemmatize(word)\n",
    "            \n",
    "            new_words.append((lemmas))\n",
    "    print(new_words)\n",
    "\n",
    "    #for word, tag in tagged_sentence:\n",
    "        #print(\"enter loop\",word, tag)\n",
    "        #if tag not in pn_tags:\n",
    "            #if tag in lemma_tags:\n",
    "                \n",
    "                \n",
    "                #new_tag = POS_tag_lookup(tag)\n",
    "                #print(\"new tag\", new_tag)\n",
    "                #new_word = (lemma.lemmatize(word, new_tag))\n",
    "                #print(\"lematized word with POS\", new_word)\n",
    "                #new_words.append(new_word)\n",
    "                #print(\"lemma\", new_words)\n",
    "            #else:\n",
    "                #new_words.append(word)\n",
    "\n",
    "    sentence = new_words\n",
    "\n",
    "\n",
    "    #sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    #print(sentence)\n",
    "\n",
    "\n",
    "    #sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    #return sentence\n",
    "\n",
    "    # Expand contractions\n",
    "    if True:\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "    \n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        sentence = [w for w in sentence if not w in stops]\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "#text = \"agreeable agrees agree agreed draws drew draw running runs run change changing changes changed Professor John in Denmark with Leo\"\n",
    "#text = \"I ate a sandwich and it was very tasty\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df['encoded_sentences'] = sentences_df['sentences'].apply(tidy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemma test\n",
    "\n",
    "text = \"the bloodied person saw some blood drawing draws drew draw running runs run change changing changes change\"\n",
    "nltk.tag.pos_tag(text.split())\n",
    "tagged_sentence = nltk.tag.pos_tag(text.split())\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ, #adjective\n",
    "    \"N\": wordnet.NOUN,#noun\n",
    "    \"V\": wordnet.VERB,#verb\n",
    "    \"R\": wordnet.ADV} #adverb\n",
    "\n",
    "return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  text=[WordNetLemmatizer().lemmatize(w, get_pos_tags(w)) for w in text]   \n",
    "  return text\n",
    "\n",
    "final_output=lemmatize_text(example)\n",
    "print (final_output)\n",
    "    \n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    sentence = (' '.join(edited_sentence))\n",
    "\n",
    "text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "sentence = [lemma.lemmatize(word) for word in text]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(sentences_df.year)\n",
    "rated_dummies = pd.get_dummies((sentences_df).year)\n",
    "sentences_df = pd.concat([sentences_df, rated_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = sentences_df[['encoded_sentences', '2010', '2012']]\n",
    "#y = sentences_df[\"col_type\"]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences_df['encoded_sentences']\n",
    "y = sentences_df['col_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(max_features= 1000, lowercase=False, tokenizer=False)\n",
    "def fake(token):\n",
    "    return token\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fake,\n",
    "    preprocessor=fake,\n",
    "    token_pattern=None)  \n",
    "\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial logistic regression \n",
    "logreg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", max_iter= 5000) #classifier \n",
    "logreg.fit(X_train, y_train) #fit the model \n",
    "logreg.score(X_train, y_train) #get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = logreg.coef_[0]\n",
    "#male is 2 and female is 0\n",
    "\n",
    "sorted_coef = sorted((zip(tfidf.get_feature_names(), coefs)), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "high_coef = sorted_coef[:10]\n",
    "low_coef = sorted_coef[-10:]\n",
    "\n",
    "print(\"highest coefs\")\n",
    "for i in high_coef: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lowest coefs\")\n",
    "for i in low_coef: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(X_train.columns, np.transpose(abs(logreg.coef_))), columns=['features', 'coef']) #use absolute values to identify biggest coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, male coefficient for neutral is higher than the female. Otherwise, as expected, the coefficients for female and male each are correspondingly high for each gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logreg.predict(X_test) #predict test \n",
    "print(metrics.accuracy_score(y_test, prediction)) #accuracy \n",
    "print(metrics.confusion_matrix(y_test, prediction)) #confusion matrix\n",
    "print(metrics.classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think with the above solution we wouldn't need this function \n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "sentences_df['male_count2'] = sentences_df['sentences'].apply(count_words, word_list=his_w)\n",
    "sentences_df['female_count2'] = sentences_df['sentences'].apply(count_words, word_list=her_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "sentences_df.loc[[7]]\n",
    "\n",
    "#there is a bug here. It weirdly seems to be double counting? The zip function is new to me though so maybe thats the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': ['This is a sample text', 'Another text example', 'One more example']})\n",
    "\n",
    "# define two lists of specific words to count\n",
    "word_list1 = ['text', 'example']\n",
    "word_list2 = ['is', 'more']\n",
    "\n",
    "def count_words(text, word_list):\n",
    "    return sum(text.count(word) for word in word_list)\n",
    "\n",
    "# use apply() to add two new columns with the counts of the specific words in each list\n",
    "df['word_count1'] = df['text'].apply(count_words, word_list=word_list1)\n",
    "df['word_count2'] = df['text'].apply(count_words, word_list=word_list2)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
