{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nytimes-scraper\n",
    "import datetime as dt\n",
    "from nytimes_scraper.nyt_api import NytApi\n",
    "#from nytimes_scraper.articles import fetch_articles_by_month, articles_to_df\n",
    "from nytimes_scraper import run_scraper, scrape_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = NytApi('FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Githubs with code: \n",
    "https://github.com/ietz/nytimes-scraper (implemented above)\n",
    "\n",
    "https://github.com/suewoon/nyt-api-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for 2020-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 1642/4240 [04:50<06:32,  6.62Article/s]"
     ]
    }
   ],
   "source": [
    "# scrape february of 2020\n",
    "article_df = scrape_month('FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv', date=dt.date(2020, 2, 1))\n",
    "\n",
    "# scrape all articles month by month\n",
    "run_scraper('FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENTS**\n",
    "I tried running the above with the api variable but it prefers the full length string for some reason. The pieces of code fully run and after loading 100% of the articles for the month it gives an error. Not sure why. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/brainyrobo/nyt-scraper/notebook (other scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export NYT_API_TOKEN='FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/brainyrobo/nyt-scraper/notebook?scriptVersionId=50372330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting console-progressbar\n",
      "  Downloading console_progressbar-1.1.2.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: console-progressbar\n",
      "  Building wheel for console-progressbar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for console-progressbar: filename=console_progressbar-1.1.2-py3-none-any.whl size=4139 sha256=1c1e9f4c333b11fbde1a815683b9f5dc4d000d6c203eea93a2a4b4180c00a076\n",
      "  Stored in directory: /Users/yolandaferreirofranchi/Library/Caches/pip/wheels/3a/63/bd/ef864ce125cac534f907d89983080597a83492c8a5cee04091\n",
      "Successfully built console-progressbar\n",
      "Installing collected packages: console-progressbar\n",
      "Successfully installed console-progressbar-1.1.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install console-progressbar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from console_progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n",
      "Start |XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX| 100.000% End\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2010\n",
    "MONTH = 1\n",
    "BASE_URL = \"https://api.nytimes.com/svc/archive/v1/{y}/{m}.json?api-key=FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv\".format(y = YEAR,m = MONTH)\n",
    "temp_data = []\n",
    "count = 0\n",
    "\n",
    "# Pick how many articles you want per month\n",
    "DATA_PER_MONTH = 5\n",
    "## Go through the years\n",
    "for y in range(2010,2011):\n",
    "\n",
    "  # in every month of that year\n",
    "  for m in range(1,11):\n",
    "    BASE_URL = \"https://api.nytimes.com/svc/archive/v1/{y}/{m}.json?api-key=FwXEDbsh0gDgF94OHOU4d9aJRjfXdyCv\".format(y = y,m = m)\n",
    "    # get all moth news data\n",
    "    all_data = requests.get(BASE_URL).json()\n",
    "    # extract the article from each news URL\n",
    "    count = 0\n",
    "    pb = ProgressBar(total=len(all_data[\"response\"][\"docs\"][:DATA_PER_MONTH]),prefix='Start', suffix='End', decimals=3, length=50, fill='X', zfill='-')\n",
    "    for temp_news in all_data[\"response\"][\"docs\"][:DATA_PER_MONTH]:\n",
    "      count += 1\n",
    "      pb.print_progress_bar(count)\n",
    "      news_body = requests.get(temp_news[\"web_url\"]).text\n",
    "      body = BeautifulSoup(news_body, 'html.parser').find(attrs={\"name\": \"articleBody\"})\n",
    "      if body != None:\n",
    "        temp_data.append({\"headline\":temp_news[\"headline\"][\"main\"],\"body\":body.get_text()})\n",
    "    temp_data = json.dumps(temp_data)\n",
    "    f = open(\"{y}-{m}.json\".format(y=y,m=m),\"w\")\n",
    "    f.write(temp_data)\n",
    "    f.close()\n",
    "    temp_data = []\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('2010-1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Use the data\n",
    "print(data)\n",
    "\n",
    "#string ends up empty? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ashleychampagne/Web-Scraping-Toolkit/blob/master/NYT-Collection-Workflow.md"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last-resort** https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
    "\n",
    "This kaggle dataset has 1M articles from multiple news sources across the topics of business, world news, science, and sports and was developed by a researcher at NYU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
