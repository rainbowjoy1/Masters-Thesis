{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import pickle\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#for RNN\n",
    "#import tensorflow as tf\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_processed_sent</th>\n",
       "      <th>male_count</th>\n",
       "      <th>female_count</th>\n",
       "      <th>Proper_noun_list</th>\n",
       "      <th>pn exists</th>\n",
       "      <th>sentences</th>\n",
       "      <th>article_id</th>\n",
       "      <th>year</th>\n",
       "      <th>col_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[plead, guilty, rap, two, count, assault, touch]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>He pleaded guilty to raping a boy under 13, a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[court, hear, assault, happen, nine, month, pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>The court heard the assaults happened over a n...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[say, would, serve, half, two, year, sentence,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Diana]</td>\n",
       "      <td>None</td>\n",
       "      <td>District Judge Diana Baker said he would serve...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[say, abuse, take, place, minimum, separate, i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>She said the abuse took place from September 2...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[say, true, scale, come, sex, education, lesso...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[Fiona]</td>\n",
       "      <td>None</td>\n",
       "      <td>Prosecutor Fiona Elder said the true scale of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778832</th>\n",
       "      <td>[think, everyone, voice, heard, election, say]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>\"You thought that everyone had had their voice...</td>\n",
       "      <td>2176246</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778833</th>\n",
       "      <td>[elect, partly, promise, would, bring, unemplo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Rouhani]</td>\n",
       "      <td>None</td>\n",
       "      <td>President Rouhani was re-elected in May partly...</td>\n",
       "      <td>2176246</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778834</th>\n",
       "      <td>[sign, agreement, six, country, include, promi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>It was after he signed an agreement with six c...</td>\n",
       "      <td>2176246</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778835</th>\n",
       "      <td>[really, felt, economic, benefit, agreement, m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>But Iranians haven't really felt the economic ...</td>\n",
       "      <td>2176246</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778840</th>\n",
       "      <td>[add, could, comparison, certain, extent, iran...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>'\"She adds that there could be comparisons to ...</td>\n",
       "      <td>2176246</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>842023 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        pre_processed_sent  male_count  \\\n",
       "2         [plead, guilty, rap, two, count, assault, touch]           3   \n",
       "3        [court, hear, assault, happen, nine, month, pe...           1   \n",
       "4        [say, would, serve, half, two, year, sentence,...           1   \n",
       "5        [say, abuse, take, place, minimum, separate, i...           0   \n",
       "6        [say, true, scale, come, sex, education, lesso...           2   \n",
       "...                                                    ...         ...   \n",
       "2778832     [think, everyone, voice, heard, election, say]           0   \n",
       "2778833  [elect, partly, promise, would, bring, unemplo...           1   \n",
       "2778834  [sign, agreement, six, country, include, promi...           2   \n",
       "2778835  [really, felt, economic, benefit, agreement, m...           1   \n",
       "2778840  [add, could, comparison, certain, extent, iran...           0   \n",
       "\n",
       "         female_count Proper_noun_list pn exists  \\\n",
       "2                   2               []      None   \n",
       "3                   0               []      None   \n",
       "4                   0          [Diana]      None   \n",
       "5                   1               []      None   \n",
       "6                   1          [Fiona]      None   \n",
       "...               ...              ...       ...   \n",
       "2778832             1               []      None   \n",
       "2778833             0        [Rouhani]      None   \n",
       "2778834             0               []      None   \n",
       "2778835             0               []      None   \n",
       "2778840             1               []      None   \n",
       "\n",
       "                                                 sentences  article_id  year  \\\n",
       "2        He pleaded guilty to raping a boy under 13, a ...           2  2018   \n",
       "3        The court heard the assaults happened over a n...           2  2018   \n",
       "4        District Judge Diana Baker said he would serve...           2  2018   \n",
       "5        She said the abuse took place from September 2...           2  2018   \n",
       "6        Prosecutor Fiona Elder said the true scale of ...           2  2018   \n",
       "...                                                    ...         ...   ...   \n",
       "2778832  \"You thought that everyone had had their voice...     2176246  2018   \n",
       "2778833  President Rouhani was re-elected in May partly...     2176246  2018   \n",
       "2778834  It was after he signed an agreement with six c...     2176246  2018   \n",
       "2778835  But Iranians haven't really felt the economic ...     2176246  2018   \n",
       "2778840  '\"She adds that there could be comparisons to ...     2176246  2018   \n",
       "\n",
       "        col_type  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              1  \n",
       "6              0  \n",
       "...          ...  \n",
       "2778832        1  \n",
       "2778833        0  \n",
       "2778834        0  \n",
       "2778835        0  \n",
       "2778840        1  \n",
       "\n",
       "[842023 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pre-processed data from pickle \n",
    "#df_10= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2010_text_wo_names.pickle\")\n",
    "#df_09= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2009_text_wo_names.pickle\")\n",
    "#df_11= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2011_text_wo_names.pickle\")\n",
    "#df_12= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2012_text_wo_names.pickle\")\n",
    "#df_13= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2013_text_wo_names.pickle\")\n",
    "#df_14= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2014_text_wo_names.pickle\")\n",
    "#df_15= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2015_text_wo_names.pickle\")\n",
    "#df_16= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2016_text_wo_names.pickle\")\n",
    "#df_17= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2017_text_wo_names.pickle\")\n",
    "df_18= pd.read_pickle(r\"/Users/yolandaferreirofranchi/Desktop/ThesisDatasets/2018_text_wo_names.pickle\")\n",
    "df_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_count(male_col, female_col):\n",
    "    if female_col > male_col and male_col == 0:\n",
    "        return 1\n",
    "    elif male_col> female_col and female_col ==0: \n",
    "        return 0\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    533765\n",
       "1.0    263341\n",
       "Name: col_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply function to only get rows with an absolute count \n",
    "df_18['col_type'] = df_18.apply(lambda row: absolute_count(row['male_count'], row['female_count']),axis=1)\n",
    "\n",
    "#remove nulls \n",
    "df_18= df_18[df_18[\"col_type\"].notnull()]\n",
    "\n",
    "#DOC: number of male and female columns\n",
    "df_18[\"col_type\"].value_counts()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer\n",
    "def fake(token):\n",
    "    return token\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fake,\n",
    "    preprocessor=fake,\n",
    "    token_pattern=None)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What the LR model does-* LR estimates the probability of an instance belonging to the positive class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec to dos\n",
    "#import gensim\n",
    "#from gensim.models import Word2Vec\n",
    "#file_w2vec= (r\"/Users/yolandaferreirofranchi/Desktop/GoogleNews-vectors-negative300.bin\") #yolanda's path \n",
    "#model_w2v = gensim.models.KeyedVectors.load_word2vec_format(file_w2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_year(df, target_col, text_col):\n",
    "    #start timer \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #split data \n",
    "    X = df[text_col].apply(lambda x: str(x))\n",
    "    y = df[target_col]\n",
    "\n",
    "    #train test split\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_transformed = tfidf.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # define the hyperparameters to search over\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag' 'saga'], #removed liblinear as it is for small + medium datasets & NOT for sparse data\n",
    "        'class_weight': ['balanced', {0: 0.3, 1: 0.7}],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    #the classifier \n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    #create a GridsearchCV object \n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_clf = LogisticRegression(**grid_search.best_params_)\n",
    "\n",
    "    #run the classifier \n",
    "    best_clf.fit(X_train, y_train)\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    #performance \n",
    "    accuracy = best_clf.score(X_test, y_test) #evaluate on test set\n",
    "    class_report = classification_report(y_test, y_pred, zero_division = 0)\n",
    "    #results = {'accuracy': accuracy, 'classification_report': class_report}\n",
    "    #print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    #print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "    #coefficients\n",
    "    coefs = best_clf.coef_[0]\n",
    "    sorted_coef = sorted((zip(tfidf.get_feature_names_out(), coefs)), key = lambda x: x[1], reverse=True)\n",
    "    high_coef = sorted_coef[:200]\n",
    "    low_coef = sorted_coef[-200:]\n",
    "    \n",
    "    df_high_coef = pd.DataFrame(high_coef, columns=['feature', 'coef'])\n",
    "    df_low_coef = pd.DataFrame(low_coef, columns=['feature', 'coef'])\n",
    "\n",
    "\n",
    "    #write word probability list\n",
    "    #feature_indices = {feature: idx for idx, feature in enumerate(tfidf.get_feature_names_out())}\n",
    "    #probas = best_clf.predict_proba(X_transformed)  # predict probabilities of positive class\n",
    "    #positive_probas = probas[:, 1]\n",
    "    #probabilities = [None] * len(feature_indices)\n",
    "    #for feature, index in feature_indices.items():\n",
    "        #probabilities[index] = f\"{feature}: {(X_transformed[:, index].toarray() * positive_probas).mean()}\"\n",
    "\n",
    "    #save model \n",
    "    with open('results.pkl', 'wb') as f:\n",
    "        pickle.dump({'accuracy': accuracy, 'report': class_report}, f)\n",
    "\n",
    "    #end timer \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExecution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return df_high_coef, df_low_coef, best_clf, class_report #df_probs_top"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time: 897.36 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(         feature       coef\n",
       " 0        harrier  10.866663\n",
       " 1     breastfeed   9.363706\n",
       " 2    suffragette   9.250195\n",
       " 3          hijab   8.659470\n",
       " 4         chibok   8.520039\n",
       " ..           ...        ...\n",
       " 195         veil   5.251339\n",
       " 196         grid   5.246982\n",
       " 197   markievicz   5.246095\n",
       " 198    gaitskell   5.244970\n",
       " 199    drunkenly   5.240922\n",
       " \n",
       " [200 rows x 2 columns],\n",
       "            feature      coef\n",
       " 0           goring -4.937734\n",
       " 1          lateral -4.940728\n",
       " 2             jake -4.946211\n",
       " 3            tinie -4.948090\n",
       " 4           orwell -4.951249\n",
       " ..             ...       ...\n",
       " 195          moshe -7.080489\n",
       " 196           lula -7.243151\n",
       " 197       hydrogen -7.247649\n",
       " 198  authoritarian -8.322096\n",
       " 199       prostate -8.490432\n",
       " \n",
       " [200 rows x 2 columns],\n",
       " LogisticRegression(C=10, class_weight='balanced', random_state=42,\n",
       "                    solver='newton-cg'),\n",
       " '              precision    recall  f1-score   support\\n\\n         0.0       0.81      0.69      0.75    106502\\n         1.0       0.52      0.67      0.59     52920\\n\\n    accuracy                           0.69    159422\\n   macro avg       0.67      0.68      0.67    159422\\nweighted avg       0.71      0.69      0.69    159422\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probs_18 = logistic_regression_year(df_18, 'col_type', 'pre_processed_sent')\n",
    "df_probs_18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpreting Performance*\n",
    "\n",
    "The LR model is WAY better in terms of precision, recall, and f1-score at predicting the negative class - i.e. male. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open model performance metrics \n",
    "with open('results_18.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "accuracy = results['accuracy']\n",
    "report = results['report']\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Classification report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>coef_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prostate</td>\n",
       "      <td>-8.490432</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>authoritarian</td>\n",
       "      <td>-8.322096</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hydrogen</td>\n",
       "      <td>-7.247649</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lula</td>\n",
       "      <td>-7.243151</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moshe</td>\n",
       "      <td>-7.080489</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alfie</td>\n",
       "      <td>-7.048919</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>knighthood</td>\n",
       "      <td>-7.016132</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bedsore</td>\n",
       "      <td>-6.998203</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mcpartlin</td>\n",
       "      <td>-6.976248</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sedition</td>\n",
       "      <td>-6.832047</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>distend</td>\n",
       "      <td>-6.764916</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>businessmen</td>\n",
       "      <td>-6.691713</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cartel</td>\n",
       "      <td>-6.657280</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bannon</td>\n",
       "      <td>-6.636644</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chubby</td>\n",
       "      <td>-6.544384</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>maldives</td>\n",
       "      <td>-6.460224</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>joel</td>\n",
       "      <td>-6.452768</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>privatisation</td>\n",
       "      <td>-6.422367</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>herbal</td>\n",
       "      <td>-6.371584</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>denuclearisation</td>\n",
       "      <td>-6.285497</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pontiff</td>\n",
       "      <td>-6.218117</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bipartisan</td>\n",
       "      <td>-6.205917</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>populism</td>\n",
       "      <td>-6.200210</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>jon</td>\n",
       "      <td>-6.193572</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>admiral</td>\n",
       "      <td>-6.166566</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gareth</td>\n",
       "      <td>-6.162978</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>minke</td>\n",
       "      <td>-6.135688</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>howard</td>\n",
       "      <td>-6.096463</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gomi</td>\n",
       "      <td>-6.087135</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kyle</td>\n",
       "      <td>-6.074598</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>kaluuya</td>\n",
       "      <td>-5.991742</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>windmill</td>\n",
       "      <td>-5.985142</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>err</td>\n",
       "      <td>-5.977563</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanker</td>\n",
       "      <td>-5.961186</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>agri</td>\n",
       "      <td>-5.959714</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>meyer</td>\n",
       "      <td>-5.855307</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>arthur</td>\n",
       "      <td>-5.838624</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dictator</td>\n",
       "      <td>-5.810246</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>jeopardy</td>\n",
       "      <td>-5.809673</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>abruption</td>\n",
       "      <td>-5.804239</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>saudis</td>\n",
       "      <td>-5.803908</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>stubble</td>\n",
       "      <td>-5.763057</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>upturned</td>\n",
       "      <td>-5.753176</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>aboriginal</td>\n",
       "      <td>-5.734331</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>cone</td>\n",
       "      <td>-5.733325</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>henry</td>\n",
       "      <td>-5.725423</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>vowel</td>\n",
       "      <td>-5.724072</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>vassal</td>\n",
       "      <td>-5.715838</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>fatherhood</td>\n",
       "      <td>-5.662841</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>quoting</td>\n",
       "      <td>-5.660280</td>\n",
       "      <td>lowest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature      coef coef_type\n",
       "0           prostate -8.490432    lowest\n",
       "1      authoritarian -8.322096    lowest\n",
       "2           hydrogen -7.247649    lowest\n",
       "3               lula -7.243151    lowest\n",
       "4              moshe -7.080489    lowest\n",
       "5              alfie -7.048919    lowest\n",
       "6         knighthood -7.016132    lowest\n",
       "7            bedsore -6.998203    lowest\n",
       "8          mcpartlin -6.976248    lowest\n",
       "9           sedition -6.832047    lowest\n",
       "10           distend -6.764916    lowest\n",
       "11       businessmen -6.691713    lowest\n",
       "12            cartel -6.657280    lowest\n",
       "13            bannon -6.636644    lowest\n",
       "14            chubby -6.544384    lowest\n",
       "15          maldives -6.460224    lowest\n",
       "16              joel -6.452768    lowest\n",
       "17     privatisation -6.422367    lowest\n",
       "18            herbal -6.371584    lowest\n",
       "19  denuclearisation -6.285497    lowest\n",
       "20           pontiff -6.218117    lowest\n",
       "21        bipartisan -6.205917    lowest\n",
       "22          populism -6.200210    lowest\n",
       "23               jon -6.193572    lowest\n",
       "24           admiral -6.166566    lowest\n",
       "25            gareth -6.162978    lowest\n",
       "26             minke -6.135688    lowest\n",
       "27            howard -6.096463    lowest\n",
       "28              gomi -6.087135    lowest\n",
       "29              kyle -6.074598    lowest\n",
       "30           kaluuya -5.991742    lowest\n",
       "31          windmill -5.985142    lowest\n",
       "32               err -5.977563    lowest\n",
       "33            tanker -5.961186    lowest\n",
       "34              agri -5.959714    lowest\n",
       "35             meyer -5.855307    lowest\n",
       "36            arthur -5.838624    lowest\n",
       "37          dictator -5.810246    lowest\n",
       "38          jeopardy -5.809673    lowest\n",
       "39         abruption -5.804239    lowest\n",
       "40            saudis -5.803908    lowest\n",
       "41           stubble -5.763057    lowest\n",
       "42          upturned -5.753176    lowest\n",
       "43        aboriginal -5.734331    lowest\n",
       "44              cone -5.733325    lowest\n",
       "45             henry -5.725423    lowest\n",
       "46             vowel -5.724072    lowest\n",
       "47            vassal -5.715838    lowest\n",
       "48        fatherhood -5.662841    lowest\n",
       "49           quoting -5.660280    lowest"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create DF of highest coef\n",
    "highest_coef_18 = pd.DataFrame(df_probs_18[0])\n",
    "highest_coef_18[\"coef_type\"] = \"highest\"\n",
    "\n",
    "#create DF of lowest lowest coef manipulation \n",
    "lowest_coef_18 = pd.DataFrame(df_probs_18[1]) \n",
    "lowest_coef_18 = lowest_coef_18.sort_values(by = [\"coef\"], ascending = True).reset_index(drop = True) #absolute lowest value \n",
    "lowest_coef_18[\"coef_type\"] = \"lowest\"\n",
    "lowest_coef_18.head(50)\n",
    "\n",
    "#stacked_18 = pd.concat([highest_coef_18, lowest_coef_18])\n",
    "#stacked_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save DF as pickle file per year \n",
    "lowest_coef_18.to_pickle('RESULTS18_coef_low.pickle')\n",
    "highest_coef_18.to_pickle('RESULTS18_coef_high.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of coefficient results**\n",
    "\"winner\" is one of the independent variables in the model and its coefficient value is 1.2865737872946597. This means that a one unit increase in the value of the \"winner\" variable will increase the log-odds of the positive class (e.g. \"female\" if the logistic regression model is binary and predicting gender) by the corresponding coefficient value, while holding all other variables constant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Probability Analysis** \n",
    "\n",
    "#make sure to un-comment the last two lines of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access list of top words \n",
    "word_prob_10 = pd.DataFrame(df_probs_10[3]) \n",
    "word_prob_10 = word_prob_10.sort_values([\"probability\"], ascending= False)\n",
    "\n",
    "#save list of words (highest to lowest probability of being female) for year: \n",
    "word_prob_10.to_pickle('RESULTS09_words_prob.pickle')\n",
    "word_prob_10.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the predicted probabilities in LR:** \n",
    "The predicted probabilities of the logistic regression model tell us the probability that the input data belongs to the positive class - in this case the female class as we attributed it a value = 1 in binary log reg. Hence, for each word, we get a list of a word/feature and the probability that it is female. \n",
    "\n",
    "These predicted probabilities can be interpreted as the confidence level of the model in its prediction. For example, a predicted probability of 0.8 for a positive class means that the model is 80% confident that the sample belongs to the positive class. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "def svm_year(df, target_col, text_col):\n",
    "    #start timer \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #split data \n",
    "    X = df[text_col].apply(lambda x: str(x))\n",
    "    y = df[target_col]\n",
    "\n",
    "    #train test split\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_transformed = tfidf.fit_transform(X).toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # define the hyperparameters to search over\n",
    "    param_grid = {\n",
    "        'C': [.001, .01, .1, 1, 10, 100],\n",
    "        #'kernel': default('rbf')',\n",
    "        'degree': [2,3,4],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        #'class_weight': [None],\n",
    "        #'random_state': [42]\n",
    "    }\n",
    "\n",
    "    #the classifier \n",
    "    clf = SVC()\n",
    "\n",
    "    #create a GridsearchCV object \n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_clf = SVC(**grid_search.best_params_)\n",
    "\n",
    "    #run the classifier \n",
    "    best_clf.fit(X_train, y_train)\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    #performance \n",
    "    accuracy = best_clf.score(X_test, y_test) #evaluate on test set\n",
    "    class_report = classification_report(y_test, y_pred, zero_division = 0)\n",
    "    results = {'accuracy': accuracy, 'classification_report': class_report}\n",
    "    #print(f\"Year: {year}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "    #get important features (equivalent of coefficients in logreg)\n",
    "    result_clf = permutation_importance(best_clf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    importance_scores = result_clf.importances_mean\n",
    "    importance_scores_df = pd.DataFrame(importance_scores, columns=['tbd'])\n",
    "\n",
    "    for i in range(len(importance_scores)):\n",
    "        print(\"Feature {}: Importance score = {:.3f}\".format(i, importance_scores[i]))\n",
    "    \n",
    "    #print probability results \n",
    "    #feature_indices = {feature: idx for idx, feature in enumerate(tfidf.get_feature_names_out())}\n",
    "    #probas = best_clf.predict_proba(X_transformed)  # predict probabilities of positive class\n",
    "    #positive_probas = probas[:, 1]\n",
    "\n",
    "    #print(f\"\\nProbability of the Following Words Being Female:\")\n",
    "    #feature_prob_dict = {}\n",
    "    #for feature, index in feature_indices.items():\n",
    "        #proba = (X_transformed[:, index] * positive_probas).mean()\n",
    "        #feature_prob_dict[feature] = proba\n",
    "        #print(f\"{feature}: {proba}\")\n",
    "    \n",
    "    #create a dataframe with the data: \n",
    "    #df_probs = pd.DataFrame.from_dict(feature_prob_dict, orient='index', columns=['probability'])\n",
    "    \n",
    "    #return df_probs #DF of probability for each word being female by year\n",
    "\n",
    "    #end timer \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExecution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return importance_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_year(df_09, 'col_type', 'pre_processed_sent') #takes more than 1 hour to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the SVM model \n",
    "import joblib\n",
    "model, accuracy, class_report = svm_year(df_09, 'col_type', 'pre_processed_sent')\n",
    "\n",
    "# Save the model and performance metrics\n",
    "joblib.dump(model, 'model_svm.pkl')\n",
    "with open('performance_metrics.txt', 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "    f.write(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {' ': ['Logistic Regression', 'SVM', 'NN'],\n",
    "           'Accuracy': [0.66, 0.70, 0],\n",
    "           'Time (s)': [54.39, 3540, 0]}\n",
    "\n",
    "# Create a pandas dataframe from the dictionary\n",
    "df_mod_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Set the index of the dataframe to the Kernel column\n",
    "df_mod_results.set_index(' ', inplace=True)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df_mod_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
