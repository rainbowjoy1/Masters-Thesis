{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import pickle\n",
    "import os\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = ['2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n",
    "file_path = \"C:/Users/danie/Desktop/Masters Thesis/New Clean Data for Log Reg/\"\n",
    "save_path = \"C:/Users/danie/Desktop/Masters Thesis/Log Reg Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_count(male_col, female_col):\n",
    "    if female_col > male_col and male_col == 0:\n",
    "        return 1\n",
    "    elif male_col> female_col and female_col ==0: \n",
    "        return 0\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer\n",
    "def fake(token):\n",
    "    return token\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=fake,\n",
    "    preprocessor=fake,\n",
    "    token_pattern=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_year(year, df, target_col, text_col):\n",
    "    #start timer \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #split data \n",
    "    X = df[text_col].apply(lambda x: str(x))\n",
    "    y = df[target_col]\n",
    "\n",
    "    #train test split\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_transformed = tfidf.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # define the hyperparameters to search over\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag' 'saga'], #removed liblinear as it is for small + medium datasets & NOT for sparse data\n",
    "        'class_weight': ['balanced', {0: 0.3, 1: 0.7}],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    #the classifier \n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    #create a GridsearchCV object \n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_clf = LogisticRegression(**grid_search.best_params_)\n",
    "\n",
    "    #run the classifier \n",
    "    best_clf.fit(X_train, y_train)\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "\n",
    "    #performance \n",
    "    accuracy = best_clf.score(X_test, y_test) #evaluate on test set\n",
    "    class_report = classification_report(y_test, y_pred, zero_division = 0)\n",
    "    #results = {'accuracy': accuracy, 'classification_report': class_report}\n",
    "    #print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    #print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "    #coefficients\n",
    "    coefs = best_clf.coef_[0]\n",
    "    sorted_coef = sorted((zip(tfidf.get_feature_names_out(), coefs)), key = lambda x: x[1], reverse=True)\n",
    "    high_coef = sorted_coef[:1000]\n",
    "    low_coef = sorted_coef[-1000:]\n",
    "    \n",
    "    df_high_coef = pd.DataFrame(high_coef, columns=['feature', 'coef'])\n",
    "    df_low_coef = pd.DataFrame(low_coef, columns=['feature', 'coef'])\n",
    "\n",
    "    #save model \n",
    "    with open(save_path + year + '_results.pkl', 'wb') as f:\n",
    "        pickle.dump({'model': best_clf, 'tfidf': tfidf, 'accuracy': accuracy, 'report': class_report}, f)\n",
    "\n",
    "    #end timer \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nExecution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return df_high_coef, df_low_coef, best_clf, class_report #df_probs_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle(r\"C:\\Users\\danie\\Desktop\\Masters Thesis\\New Clean Data for Log Reg\\2012_final_rnn.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_log_reg(year_list):\n",
    "\n",
    "\n",
    "    for year in year_list:\n",
    "        print(year, \"model:\")\n",
    "        df= pd.read_pickle(file_path + year + \"_final_rnn.pickle\")\n",
    "\n",
    "        print(\"Size before removal\", len(df. index))\n",
    "        \n",
    "        #apply function to only get rows with an absolute count \n",
    "        df['col_type'] = df.apply(lambda row: absolute_count(row['male_count'], row['female_count']),axis=1)\n",
    "\n",
    "        #remove nulls \n",
    "        df = df[df[\"col_type\"].notnull()]\n",
    "\n",
    "        print(\"Size after removal\", len(df. index))\n",
    "\n",
    "        print( \"class distribution\", df[\"col_type\"].value_counts())\n",
    "\n",
    "        df_prob = logistic_regression_year(year, df, 'col_type', 'pre_processed_sent')\n",
    "\n",
    "        with open(save_path + year + '_logreg_model.pkl', 'wb') as handle:\n",
    "            pickle.dump(df_prob, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(save_path + year + '_results.pkl', 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        \n",
    "        accuracy = results['accuracy']\n",
    "        report = results['report']\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Classification report:\\n{report}\")\n",
    "\n",
    "\n",
    "        highest_coef = pd.DataFrame(df_prob[0])\n",
    "        highest_coef[\"coef_type\"] = \"highest\"\n",
    "        highest_coef[\"year\"] = year\n",
    "\n",
    "        highest_coef.to_pickle(save_path + year + \"highest_coef\")\n",
    "\n",
    "        lowest_coef = pd.DataFrame(df_prob[1]) \n",
    "        lowest_coef = lowest_coef.sort_values(by = [\"coef\"], ascending = True).reset_index(drop = True) #absolute lowest value \n",
    "        lowest_coef[\"coef_type\"] = \"lowest\" #coef type\n",
    "        lowest_coef[\"year\"] = year #year \n",
    "        lowest_coef.to_pickle(save_path + year + \"lowest_coef\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 model:\n",
      "Size before removal 911347\n",
      "Size after removal 874527\n",
      "class distribution 0.0    653092\n",
      "1.0    221435\n",
      "Name: col_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "yearly_log_reg(year_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
